{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f2a98d-591d-45f4-a8d0-eb5f090c974a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\nCSV 10M  rows: 10,000,000\nPP  10M  rows: 10,000,000\nPP  100M rows: 100,000,000\n\n============================================================\nTASK 1: Filter + Aggregation\n============================================================\n  CSV  10M  Task1                     → 9.8 seconds\n  PP   10M  Task1                     → 1.7 seconds\n  PP   100M Task1                     → 18.43 seconds\n\n  Speedup (CSV → PP 10M)  : 5.8x faster\n  Scaling (10M → 100M PP) : 10.8x (expect ~10x for linear)\n\n============================================================\nSTRESS 1: High-Cardinality GroupBy (4 dimensions)\n============================================================\n  CSV  10M  Stress1                   → 8.61 seconds\n  PP   10M  Stress1                   → 2.71 seconds\n  PP   100M Stress1                   → 34.06 seconds\n\n  Speedup (CSV → PP 10M)  : 3.2x faster\n\n============================================================\nSTRESS 2: Window Function — Row Number per user_id\n============================================================\n  CSV  10M  Stress2                   → 5.62 seconds\n  PP   10M  Stress2                   → 1.15 seconds\n  PP   100M Stress2                   → 3.68 seconds\n\n  Speedup (CSV → PP 10M)  : 4.9x faster\n\n============================================================\nNUCLEAR: Repartition(800) + Window + GroupBy (2 shuffles)\n============================================================\n  CSV  10M  Nuclear                   → 9.88 seconds\n  PP   10M  Nuclear                   → 4.91 seconds\n  PP   100M Nuclear                   → 33.03 seconds\n\n  Speedup (CSV → PP 10M)  : 2.0x faster\n\n============================================================\nBENCHMARK SUMMARY\n============================================================\nWorkload                               CSV 10M     PP 10M    PP 100M\n-----------------------------------------------------------------\nTask1  (Filter + Agg)                      9.8        1.7      18.43\nStress1 (High-Card GroupBy)               8.61       2.71      34.06\nStress2 (Window Function)                 5.62       1.15       3.68\nNuclear (Double Shuffle)                  9.88       4.91      33.03\n============================================================\n✅ Benchmark Complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 03_Benchmark_Suite\n",
    "# Purpose: Pure timing benchmarks — CSV vs 10M PP vs 100M PP\n",
    "#          Task1: Filter + Aggregation\n",
    "#          Stress1: High-Cardinality GroupBy\n",
    "#          Stress2: Window Function (Row Number)\n",
    "#          Nuclear: Double Shuffle (Repartition + Window + Agg)\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "from pyspark.sql.functions import col, year, month, row_number, sum as _sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ── Config ───────────────────────────────────────────────────\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 400)\n",
    "\n",
    "CSV_PATH          = \"/Volumes/workspace/default/raw_data/ecommerce_10M_55cols.csv\"\n",
    "PARQUET_10M_PATH  = \"/Volumes/workspace/default/raw_data/ecommerce_parquet\"\n",
    "PARQUET_100M_PATH = \"/Volumes/workspace/default/raw_data/ecommerce_100M_parquet\"\n",
    "\n",
    "# ── Load All Datasets ────────────────────────────────────────\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "df_csv_10M  = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(CSV_PATH)\n",
    "df_pp_10M   = spark.read.parquet(PARQUET_10M_PATH)\n",
    "df_pp_100M  = spark.read.parquet(PARQUET_100M_PATH)\n",
    "\n",
    "print(f\"CSV 10M  rows: {df_csv_10M.count():,}\")\n",
    "print(f\"PP  10M  rows: {df_pp_10M.count():,}\")\n",
    "print(f\"PP  100M rows: {df_pp_100M.count():,}\")\n",
    "\n",
    "# ── Benchmark Wrapper ────────────────────────────────────────\n",
    "def benchmark(label, func):\n",
    "    start    = time.time()\n",
    "    result   = func()\n",
    "    duration = round(time.time() - start, 2)\n",
    "    print(f\"  {label:<35} → {duration} seconds\")\n",
    "    return duration\n",
    "\n",
    "# ── Task 1: Filter + Multi-Column Aggregation ────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TASK 1: Filter + Aggregation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def task1(df):\n",
    "    df_temp = df\n",
    "    if \"year\" not in df.columns:\n",
    "        df_temp = df_temp \\\n",
    "            .withColumn(\"year\",  year(col(\"order_date\"))) \\\n",
    "            .withColumn(\"month\", month(col(\"order_date\")))\n",
    "    return df_temp \\\n",
    "        .filter((col(\"year\") >= 2023) & (col(\"month\") >= 6) & (col(\"discount\") > 500)) \\\n",
    "        .groupBy(\"city\", \"category\") \\\n",
    "        .agg({\"final_price\": \"sum\", \"quantity\": \"sum\"}) \\\n",
    "        .count()\n",
    "\n",
    "t1_csv  = benchmark(\"CSV  10M  Task1\", lambda: task1(df_csv_10M))\n",
    "t1_pp10 = benchmark(\"PP   10M  Task1\", lambda: task1(df_pp_10M))\n",
    "t1_pp100= benchmark(\"PP   100M Task1\", lambda: task1(df_pp_100M))\n",
    "\n",
    "print(f\"\\n  Speedup (CSV → PP 10M)  : {round(t1_csv / t1_pp10, 1)}x faster\")\n",
    "print(f\"  Scaling (10M → 100M PP) : {round(t1_pp100 / t1_pp10, 1)}x (expect ~10x for linear)\")\n",
    "\n",
    "# ── Stress 1: High-Cardinality GroupBy ──────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STRESS 1: High-Cardinality GroupBy (4 dimensions)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def stress1(df):\n",
    "    return df \\\n",
    "        .groupBy(\"city\", \"category\", \"payment_method\", \"product_id\") \\\n",
    "        .agg({\"final_price\": \"sum\"}) \\\n",
    "        .count()\n",
    "\n",
    "s1_csv  = benchmark(\"CSV  10M  Stress1\", lambda: stress1(df_csv_10M))\n",
    "s1_pp10 = benchmark(\"PP   10M  Stress1\", lambda: stress1(df_pp_10M))\n",
    "s1_pp100= benchmark(\"PP   100M Stress1\", lambda: stress1(df_pp_100M))\n",
    "\n",
    "print(f\"\\n  Speedup (CSV → PP 10M)  : {round(s1_csv / s1_pp10, 1)}x faster\")\n",
    "\n",
    "# ── Stress 2: Window Function (Row Number per User) ──────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STRESS 2: Window Function — Row Number per user_id\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def stress2(df):\n",
    "    window = Window.partitionBy(\"user_id\").orderBy(col(\"final_price\").desc())\n",
    "    return df.withColumn(\"rank\", row_number().over(window)).count()\n",
    "\n",
    "s2_csv  = benchmark(\"CSV  10M  Stress2\", lambda: stress2(df_csv_10M))\n",
    "s2_pp10 = benchmark(\"PP   10M  Stress2\", lambda: stress2(df_pp_10M))\n",
    "s2_pp100= benchmark(\"PP   100M Stress2\", lambda: stress2(df_pp_100M))\n",
    "\n",
    "print(f\"\\n  Speedup (CSV → PP 10M)  : {round(s2_csv / s2_pp10, 1)}x faster\")\n",
    "\n",
    "# ── Nuclear: Double-Shuffle Stress Test ──────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NUCLEAR: Repartition(800) + Window + GroupBy (2 shuffles)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def nuclear_stress(df):\n",
    "    df2      = df.select(\"user_id\",\"product_id\",\"city\",\"category\",\"payment_method\",\"final_price\")\n",
    "    shuffled = df2.repartition(800, \"user_id\")\n",
    "    window   = Window.partitionBy(\"user_id\").orderBy(col(\"final_price\").desc())\n",
    "    ranked   = shuffled.withColumn(\"rank\", row_number().over(window))\n",
    "    result   = ranked.groupBy(\"city\",\"category\",\"payment_method\").agg(_sum(\"final_price\"))\n",
    "    return result.count()\n",
    "\n",
    "n_csv   = benchmark(\"CSV  10M  Nuclear\", lambda: nuclear_stress(df_csv_10M))\n",
    "n_pp10  = benchmark(\"PP   10M  Nuclear\", lambda: nuclear_stress(df_pp_10M))\n",
    "n_pp100 = benchmark(\"PP   100M Nuclear\", lambda: nuclear_stress(df_pp_100M))\n",
    "\n",
    "print(f\"\\n  Speedup (CSV → PP 10M)  : {round(n_csv / n_pp10, 1)}x faster\")\n",
    "\n",
    "# ── Final Summary ─────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Workload':<35} {'CSV 10M':>10} {'PP 10M':>10} {'PP 100M':>10}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Task1  (Filter + Agg)':<35} {t1_csv:>10} {t1_pp10:>10} {t1_pp100:>10}\")\n",
    "print(f\"{'Stress1 (High-Card GroupBy)':<35} {s1_csv:>10} {s1_pp10:>10} {s1_pp100:>10}\")\n",
    "print(f\"{'Stress2 (Window Function)':<35} {s2_csv:>10} {s2_pp10:>10} {s2_pp100:>10}\")\n",
    "print(f\"{'Nuclear (Double Shuffle)':<35} {n_csv:>10} {n_pp10:>10} {n_pp100:>10}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ Benchmark Complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Benchmark_Suite",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}