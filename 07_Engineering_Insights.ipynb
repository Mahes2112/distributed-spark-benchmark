{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a4e754-5593-45bd-b92a-76d13cfde69c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\nDISTRIBUTED SPARK BENCHMARK — ENGINEERING INSIGHTS\nDatabricks Serverless | Photon Engine | 55-Column eCommerce Dataset\n======================================================================\n\n╔══════════════════════════════════════════════════════════════════════╗\n║                    BENCHMARK RESULTS (REAL DATA)                     ║\n╠══════════════════════════╦═══════════╦══════════╦═══════════╦════════╣\n║  Workload                ║  CSV 10M  ║  PP 10M  ║  PP 100M  ║ Speed  ║\n╠══════════════════════════╬═══════════╬══════════╬═══════════╬════════╣\n║  Task1: Filter + Agg     ║    9.80s  ║   1.70s  ║   18.43s  ║  5.8x  ║\n║  Stress1: Hi-Card GroupBy║    8.61s  ║   2.71s  ║   34.06s  ║  3.2x  ║\n║  Stress2: Window Func    ║    5.62s  ║   1.15s  ║    3.68s  ║  4.9x  ║\n║  Nuclear: 2x Shuffle     ║    9.88s  ║   4.91s  ║   33.03s  ║  2.0x  ║\n╠══════════════════════════╬═══════════╬══════════╬═══════════╬════════╣\n║  Scalability: FilterAgg  ║    8.73s  ║   0.77s  ║    1.80s  ║ 11.3x  ║\n║  Scalability: Full Scan  ║    7.71s  ║   1.40s  ║    5.27s  ║  5.5x  ║\n║  Scalability: Window     ║    5.63s  ║   1.33s  ║    3.80s  ║  4.2x  ║\n╠══════════════════════════╬═══════════╬══════════╬═══════════╬════════╣\n║  AQE: 200 partitions     ║      —    ║     —    ║    5.18s  ║   —    ║\n║  AQE: 400 partitions     ║      —    ║     —    ║    5.22s  ║   —    ║\n╠══════════════════════════╩═══════════╩══════════╩═══════════╩════════╣\n║  Storage: CSV=3.79GB | Parquet=1.25GB | Compression=3.02x            ║\n╚══════════════════════════════════════════════════════════════════════╝\n\n======================================================================\nINSIGHT 1 — FORMAT IS THE ROOT BOTTLENECK, NOT DATA VOLUME\n======================================================================\n\nYOUR DATA:\n  CSV  10M  Filter+Agg → 9.80s\n  PP   10M  Filter+Agg → 1.70s   (5.8x faster, same row count)\n  PP   100M Filter+Agg → 18.43s  (10x more data, 10.8x time — near-linear)\n\nWHAT THIS PROVES:\n  The difference between CSV and Parquet at identical row count (10M)\n  is NOT about compute power — it's about how much data Spark has to\n  physically read from storage.\n\n  CSV forces Spark to:\n    1. Read all 3.79 GB from disk (every row, every column)\n    2. Parse text byte by byte for type inference\n    3. Apply year() extraction at runtime on every single row\n    4. Filter AFTER reading everything\n\n  Parquet allows Spark to:\n    1. Read only 1.25 GB total (3.02x compression)\n    2. Skip non-2024 year partitions entirely (folder-level skip)\n    3. Read only the 2 required columns (96.36% IO reduction)\n    4. Filter BEFORE reading data into memory\n\nENGINEER'S TAKEAWAY:\n  When optimizing data pipelines, the first question isn't\n  \"how much compute do we have?\" — it's \"how much data are we\n  forcing the engine to read?\" Format choice solves that at the source.\n\n======================================================================\nINSIGHT 2 — PARTITION PRUNING BEATS LINEAR SCALING\n======================================================================\n\nYOUR DATA:\n  PP 10M  Filter+Agg (year=2024) → 0.77s\n  PP 100M Filter+Agg (year=2024) → 1.80s\n  Scaling factor: 2.34x for 10x data\n\nWHAT THIS PROVES:\n  If Spark processed ALL data linearly, 100M should take ~7.7s (10x of 0.77s).\n  It actually took 1.80s — only 2.34x.\n\n  Why? Because the year=2024 filter is a PARTITION filter, not a row filter.\n  Spark navigates the folder structure like this:\n\n    ecommerce_parquet/\n      year=2021/  ← SKIPPED entirely (not opened)\n      year=2022/  ← SKIPPED entirely\n      year=2023/  ← SKIPPED entirely\n      year=2024/  ← ONLY this folder is read\n\n  In the 100M dataset (10x replicas of 10M), Spark still only reads\n  the year=2024 partition, which is the same relative fraction.\n  This is why scaling is sub-linear — the data growth is real,\n  but the SCAN growth is controlled by partition design.\n\nPHYSICAL PLAN PROOF (from your 02_Optimization_Validation output):\n  PartitionFilters: [isnotnull(year#13525), (year#13525 = 2024)]\n  ReadSchema: struct<category:string, final_price:double>\n\n  These two lines in the Photon physical plan confirm:\n    → Spark never touched year=2021/2022/2023 folders\n    → Only 2 columns were deserialized from disk (not all 55)\n\nENGINEER'S TAKEAWAY:\n  Partition design IS query design. Choosing year/month as partition\n  keys is not just an organizational decision — it directly determines\n  whether your queries scale linearly or sub-linearly.\n\n======================================================================\nINSIGHT 3 — SHUFFLE IS WHERE DISTRIBUTED SYSTEMS ACTUALLY HURT\n======================================================================\n\nYOUR DATA (Benchmark Suite):\n  Stress1 (Hi-Cardinality 4D GroupBy):\n    PP 10M  → 2.71s\n    PP 100M → 34.06s   ← 12.6x for 10x data (SUPER-LINEAR)\n\n  Stress2 (Window Function):\n    PP 10M  → 1.15s\n    PP 100M → 3.68s    ← 3.2x for 10x data (sub-linear)\n\n  Nuclear (Repartition 800 + Window + GroupBy):\n    PP 10M  → 4.91s\n    PP 100M → 33.03s   ← 6.7x for 10x data\n\nWHAT THIS EXPLAINS:\n  Stress1 went SUPER-LINEAR (12.6x) because:\n    - groupBy(\"city\", \"category\", \"payment_method\", \"product_id\")\n    - product_id alone has ~1000 unique values\n    - 4 dimensions combined = millions of unique group keys\n    - Every row must find its group across ALL partitions\n    - Shuffle data volume grows with both row count AND cardinality\n    - At 100M rows, shuffle writes hundreds of millions of key-value pairs\n\n  Stress2 stayed sub-linear (3.2x) because:\n    - Window partitions on user_id only\n    - Each user's rows sort independently within their partition\n    - AQE coalesced small shuffle partitions automatically\n    - Less inter-node movement = less network cost\n\n  Nuclear (6.7x) sits between because:\n    - Repartition(800) forces a full shuffle first (expensive)\n    - Window function adds second shuffle\n    - But groupBy at the end has lower cardinality than Stress1\n\nKEY DISTINCTION:\n  Partition pruning reduces IO (disk problem).\n  Shuffle reduction reduces network traffic (distributed problem).\n  They are SEPARATE optimization dimensions — you need both.\n\nENGINEER'S TAKEAWAY:\n  When a query slows down 12x with 10x data, the culprit is\n  almost always cardinality explosion in the shuffle layer —\n  not the data size itself. The fix is either reducing group\n  dimensions, pre-aggregating, or using broadcast joins.\n\n======================================================================\nINSIGHT 4 — AQE ON DATABRICKS SERVERLESS MAKES MANUAL TUNING IRRELEVANT\n======================================================================\n\nYOUR DATA:\n  PP 100M — 200 shuffle partitions → 5.18s\n  PP 100M — 400 shuffle partitions → 5.22s\n  Difference: 0.04 seconds (effectively ZERO)\n\nWHAT THIS PROVES:\n  Textbook Spark tuning says: set shuffle.partitions = 2-3x your cores.\n  For 100M rows, increasing from 200 to 400 should theoretically help.\n  In practice, it made zero difference.\n\n  Why? Databricks Serverless runs Adaptive Query Execution (AQE) by default.\n  AQE observes the actual shuffle output at runtime and coalesces partitions\n  that are smaller than its target size (default 64MB per partition).\n\n  So even if you configure 400 partitions, AQE may collapse them to 50\n  based on actual data distribution — making your config irrelevant.\n\nWHAT THIS MEANS FOR PRODUCTION:\n  On Databricks Serverless, you should trust AQE and NOT manually tune\n  shuffle.partitions for standard workloads. Manual tuning only matters:\n    - When AQE is disabled (on-premise clusters, older Spark versions)\n    - When you have extreme skew AQE can't fix (salting required)\n    - When your shuffle stage has very specific partition size requirements\n\n  The fact that 400 != 200 in performance is direct evidence that\n  Serverless auto-scaling is absorbing the shuffle cost dynamically.\n\nENGINEER'S TAKEAWAY:\n  Proving something does NOT matter is as valuable as proving something\n  does. This test demonstrates you understand the difference between\n  classic Spark tuning and modern managed Spark environments.\n  That's a senior-level distinction.\n\n======================================================================\nINSIGHT 5 — COLUMN PRUNING REDUCES IO BY 96.36% (QUANTIFIED)\n======================================================================\n\nYOUR DATA (from 04_Cost_And_IO_Analysis):\n  Total columns in schema   : 55\n  Columns used in query     : 2  (category, final_price)\n  Parquet size (10M)        : 1.25 GB\n  Estimated scan after pruning: 0.05 GB\n\n  Column Scan Fraction     : 3.64%\n  IO Reduction             : 96.36%\n\n  Combined (column + year partition):\n  Combined Scan Fraction   : 0.9091%\n  Estimated Scan           : 0.01 GB\n  Total IO Reduction       : 99.09%\n\nWHAT THIS PROVES:\n  Parquet stores data in COLUMN-MAJOR format (each column in separate\n  byte ranges within the file). When Spark reads only \"category\" and\n  \"final_price\", it literally seeks to only those byte ranges on disk\n  and ignores the other 53 columns completely.\n\n  This is visible in your physical plan output:\n    ReadSchema: struct<category:string, final_price:double>\n  Only 2 columns appear in ReadSchema — the other 53 were never touched.\n\n  For CSV, ReadSchema would always include ALL columns because the\n  row-based format requires parsing every field to find the ones you want.\n\nCOST TRANSLATION:\n  CSV full scan    → $0.0185 per query (at $5/TB)\n  Parquet full scan→ $0.0061 per query\n  Optimized query  → $0.0001 per query\n  Savings vs CSV   → $0.0184 per query\n\n  At 10,000 queries/day: ~$67,160/year saved just from format + pruning.\n\nENGINEER'S TAKEAWAY:\n  Column pruning is \"free\" in Parquet — it requires zero code change.\n  Just select fewer columns. The Spark optimizer handles the rest.\n  This is one of the highest ROI optimizations in data engineering.\n\n======================================================================\nINSIGHT 6 — PHOTON VECTORIZED EXECUTION EXPLAINS SUB-SECOND TIMES\n======================================================================\n\nOBSERVATION FROM YOUR RUNS:\n  PP 10M Full Scan Agg → 1.40s    (reading 1.25GB, groupBy, sum)\n  PP 10M Window Func   → 1.15s    (shuffle + sort per user_id)\n  PP 10M Filter+Agg    → 0.77s    (partition-pruned read)\n\n  These are extraordinarily fast times for 10M rows with complex operations.\n  Standard open-source Spark on the same hardware would likely take 5-15s.\n\nWHY IS PHOTON THIS FAST?\n  All your explain() outputs end with:\n    \"The query is fully supported by Photon.\"\n\n  Photon is Databricks' native C++ vectorized execution engine that:\n    1. Processes data in SIMD batches (256/512 rows per CPU instruction)\n    2. Eliminates JVM overhead (no Java object allocation per row)\n    3. Uses CPU cache efficiently (columnar batches fit L2/L3 cache)\n    4. Integrates natively with Parquet columnar format\n\n  This is why Window Function on 10M only takes 1.15s.\n  In pure PySpark (JVM), the same operation typically takes 5-8s\n  because every row goes through Java object creation + Python GIL.\n\nWHAT THIS MEANS FOR YOUR BENCHMARK STORY:\n  Your results are Databricks Serverless + Photon results.\n  They represent the production ceiling of what Spark can do.\n  On-premise Spark without Photon would show LARGER CSV vs Parquet\n  gaps because Photon partially compensates for CSV inefficiencies.\n\nENGINEER'S TAKEAWAY:\n  Always document your runtime environment in benchmarks.\n  \"Spark on Databricks Serverless with Photon\" vs\n  \"Apache Spark 3.x on EMR\" are NOT comparable baselines.\n  Your numbers reflect a Photon-accelerated environment.\n\n======================================================================\nINSIGHT 7 — count() OVER collect() IS A DELIBERATE DESIGN CHOICE\n======================================================================\n\nYOUR BENCHMARK DESIGN:\n  Every test uses .count() as the terminal action, not .collect().\n\nWHY THIS IS CORRECT:\n  collect() behavior:\n    - Executes distributed computation on all workers ✓\n    - Then transfers ALL result rows to the driver node ✗\n    - For 100M rows, this means: driver receives ~100M Python objects\n    - Driver memory becomes the bottleneck, not cluster compute\n    - You'd be measuring Python memory allocation, not Spark performance\n\n  count() behavior:\n    - Executes distributed computation on all workers ✓\n    - Each worker returns one integer (their local count) ✓\n    - Driver receives N integers (N = number of partitions) ✓\n    - Pure distributed compute measurement — no driver bottleneck\n\nREAL IMPACT:\n  If you had used collect() in Stress1 on PP 100M:\n    - 34.06s → would likely be 5-10 MINUTES\n    - Or an OOM error on the driver\n    - Benchmark would measure driver memory pressure, not Spark compute\n\n  Your 03_Benchmark_Suite uses count() correctly throughout,\n  which is why 100M results are measurable in seconds.\n\nEDGE CASE — Price Analysis (05) uses show():\n  show() collects only the top N rows to driver (default 20).\n  This is correct for displaying results but adds ~6s overhead\n  because it triggers a full distributed sort before truncating.\n  That's why all 6 Price Analysis queries take ~6s each —\n  it's the cost of show()'s sort + collect for display.\n\nENGINEER'S TAKEAWAY:\n  Benchmark design is engineering. Using collect() in a performance\n  test is like measuring car speed with the handbrake on.\n  The choice of terminal action determines what you're actually measuring.\n\n======================================================================\nARCHITECTURE FLOW\n======================================================================\n\n  ┌─────────────────────────────────────────────────────────────┐\n  │                    YOUR PIPELINE DESIGN                      │\n  └─────────────────────────────────────────────────────────────┘\n\n  [VSCode / Local]\n       │\n       │  Generate synthetic 10M row eCommerce dataset\n       │  55 columns: transactions, users, products, logistics\n       │  Output: ecommerce_10M_55cols.csv (3.79 GB)\n       ▼\n  [Databricks Unity Volume]\n       │  /Volumes/workspace/default/raw_data/\n       │  Central storage — accessible by all Databricks notebooks\n       ▼\n  [01_Data_Load_and_Conversion]  ──── 93.2s write, 416.2s for 100M\n       │  CSV → Add year/month columns → Partitioned Parquet (10M)\n       │  CrossJoin × 10 replicas → Partitioned Parquet (100M)\n       │  Result: 1.25 GB Parquet vs 3.79 GB CSV (3.02x compression)\n       ▼\n  [02_Optimization_Validation]\n       │  explain(True) on 4 query patterns\n       │  Confirmed: PartitionFilters, ReadSchema (column pruning)\n       │  All queries: \"Fully supported by Photon\"\n       ▼\n  [03_Benchmark_Suite]           ──── 4 workloads × 3 datasets\n       │  Task1: Filter+Agg | Stress1: HiCard | Stress2: Window | Nuclear\n       │  All using count() terminal action\n       │  Proves: 2x–11x speedup CSV → Parquet\n       ▼\n  [04_Cost_And_IO_Analysis]\n       │  Quantifies: 3.02x compression, 96.36% column pruning IO reduction\n       │  99.09% combined IO reduction on optimized queries\n       │  Cost model: $0.0185 → $0.0001 per query\n       ▼\n  [05_Price_Optimization_Analysis]  ── 47.08s total, 100M records\n       │  6 business analytics on 100M dataset\n       │  Discount impact, elasticity, city revenue, prime segmentation\n       │  Proves: Production analytics viable at 100M scale in <50s\n       ▼\n  [06_Scalability_Test]\n       │  CSV baseline + 3 tests × 3 datasets + AQE partition comparison\n       │  Key finding: Partition pruning → 2.34x for 10x data (sub-linear)\n       │  AQE finding: 200 vs 400 partitions = 0.04s difference\n       ▼\n  [07_Engineering_Insights]  (this notebook)\n       │  All findings synthesized with real numbers\n       └──▶ GitHub README + Interview Prep\n\n======================================================================\nRESUME BULLET POINTS (copy-paste ready)\n======================================================================\n\n• Engineered distributed Spark pipeline on Databricks Serverless processing\n  100M+ records across 55-column eCommerce dataset stored in Unity Volume\n\n• Achieved up to 11.3x query speedup by converting CSV (3.79GB) to\n  partitioned Parquet (1.25GB), reducing storage by 66.9% and scan IO by 99%\n\n• Validated Spark physical plan optimizations (partition pruning, column\n  pruning) via explain(True), confirming PartitionFilters and minimal\n  ReadSchema in Photon execution engine\n\n• Demonstrated sub-linear scalability: partition-pruned queries on 100M rows\n  ran only 2.34x slower than 10M (10x data), proving partition design\n  eliminates linear data growth from query cost\n\n• Quantified AQE behavior on Databricks Serverless: 200 vs 400 shuffle\n  partitions produced 0.04s difference, proving managed runtime auto-tunes\n  shuffle without manual intervention\n\n• Ran 6 production-grade business analytics on 100M records in 47s total,\n  demonstrating viability of analytical workloads at scale\n\n======================================================================\nINTERVIEW TALKING POINTS — DEPTH QUESTIONS\n======================================================================\n\nQ: \"Walk me through how partition pruning works in Spark.\"\nA: When you write partitionBy(\"year\",\"month\"), Spark creates a folder\n   hierarchy: year=2024/month=6/part-*.parquet. When you filter on\n   year=2024, the Catalyst optimizer converts this to a PartitionFilter\n   before the scan stage. Spark uses the InMemoryFileIndex to list only\n   matching partition folders, never opening files in other partitions.\n   In my benchmark, this produced 2.34x scaling for 10x data growth —\n   because the fraction of data read stays constant.\n\nQ: \"Why did your Stress1 (High-Cardinality GroupBy) scale super-linearly?\"\nA: Stress1 grouped on city × category × payment_method × product_id.\n   product_id had ~1000 unique values, so the combined key space was\n   millions of unique groups. In a shuffle-based groupBy, every row must\n   be hashed to its group key and sent to the correct reducer partition.\n   At 100M rows, shuffle write volume grows multiplicatively with both\n   row count and key cardinality — which is why it went 12.6x for 10x data.\n   The fix would be pre-aggregating to reduce cardinality before groupBy.\n\nQ: \"What is AQE and what did your benchmark show about it?\"\nA: Adaptive Query Execution is Spark's runtime optimizer. It observes\n   actual shuffle output sizes (not estimates) and dynamically coalesces\n   small partitions into larger ones. My test compared 200 vs 400 shuffle\n   partitions on 100M rows — difference was 0.04 seconds. This proves\n   Databricks Serverless AQE was auto-managing partitions regardless of\n   my configuration. This is important to know — manual tuning of\n   spark.sql.shuffle.partitions is largely obsolete in managed environments.\n\nQ: \"Why did you use count() instead of collect() in benchmarks?\"\nA: collect() transfers all rows to the driver, making it a measurement\n   of Python memory allocation and driver serialization speed — not Spark\n   distributed compute. For 100M rows, collect() would cause driver OOM\n   or take minutes. count() triggers full distributed execution but returns\n   one integer per partition to the driver — pure cluster measurement.\n   This is standard benchmark design for distributed systems.\n\nQ: \"How did you generate the 100M dataset?\"\nA: I used a crossJoin replication pattern: df_10M.crossJoin(spark.range(10))\n   followed by dropping the replica column. This creates an exact 10x\n   replication with identical data distribution, which is critical for\n   fair scaling tests. Using random data generation would introduce variance\n   that makes scaling comparisons unreliable. The reproducible cross-join\n   ensures the only variable is data volume.\n\n======================================================================\n✅ Engineering Insights Complete — Project Is GitHub Ready\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 07_Engineering_Insights\n",
    "# Purpose: Structured engineering analysis built from REAL\n",
    "#          benchmark results across all 6 notebooks.\n",
    "#          Use this for GitHub README + interview prep.\n",
    "# ============================================================\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 1: COMPLETE RESULTS REGISTRY\n",
    "# All real numbers from your Databricks runs\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DISTRIBUTED SPARK BENCHMARK — ENGINEERING INSIGHTS\")\n",
    "print(\"Databricks Serverless | Photon Engine | 55-Column eCommerce Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════╗\n",
    "║                    BENCHMARK RESULTS (REAL DATA)                     ║\n",
    "╠══════════════════════════╦═══════════╦══════════╦═══════════╦════════╣\n",
    "║  Workload                ║  CSV 10M  ║  PP 10M  ║  PP 100M  ║ Speed  ║\n",
    "╠══════════════════════════╬═══════════╬══════════╬═══════════╬════════╣\n",
    "║  Task1: Filter + Agg     ║    9.80s  ║   1.70s  ║   18.43s  ║  5.8x  ║\n",
    "║  Stress1: Hi-Card GroupBy║    8.61s  ║   2.71s  ║   34.06s  ║  3.2x  ║\n",
    "║  Stress2: Window Func    ║    5.62s  ║   1.15s  ║    3.68s  ║  4.9x  ║\n",
    "║  Nuclear: 2x Shuffle     ║    9.88s  ║   4.91s  ║   33.03s  ║  2.0x  ║\n",
    "╠══════════════════════════╬═══════════╬══════════╬═══════════╬════════╣\n",
    "║  Scalability: FilterAgg  ║    8.73s  ║   0.77s  ║    1.80s  ║ 11.3x  ║\n",
    "║  Scalability: Full Scan  ║    7.71s  ║   1.40s  ║    5.27s  ║  5.5x  ║\n",
    "║  Scalability: Window     ║    5.63s  ║   1.33s  ║    3.80s  ║  4.2x  ║\n",
    "╠══════════════════════════╬═══════════╬══════════╬═══════════╬════════╣\n",
    "║  AQE: 200 partitions     ║      —    ║     —    ║    5.18s  ║   —    ║\n",
    "║  AQE: 400 partitions     ║      —    ║     —    ║    5.22s  ║   —    ║\n",
    "╠══════════════════════════╩═══════════╩══════════╩═══════════╩════════╣\n",
    "║  Storage: CSV=3.79GB | Parquet=1.25GB | Compression=3.02x            ║\n",
    "╚══════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 2: INSIGHT 1 — FORMAT IS THE ROOT BOTTLENECK\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INSIGHT 1 — FORMAT IS THE ROOT BOTTLENECK, NOT DATA VOLUME\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "YOUR DATA:\n",
    "  CSV  10M  Filter+Agg → 9.80s\n",
    "  PP   10M  Filter+Agg → 1.70s   (5.8x faster, same row count)\n",
    "  PP   100M Filter+Agg → 18.43s  (10x more data, 10.8x time — near-linear)\n",
    "\n",
    "WHAT THIS PROVES:\n",
    "  The difference between CSV and Parquet at identical row count (10M)\n",
    "  is NOT about compute power — it's about how much data Spark has to\n",
    "  physically read from storage.\n",
    "\n",
    "  CSV forces Spark to:\n",
    "    1. Read all 3.79 GB from disk (every row, every column)\n",
    "    2. Parse text byte by byte for type inference\n",
    "    3. Apply year() extraction at runtime on every single row\n",
    "    4. Filter AFTER reading everything\n",
    "\n",
    "  Parquet allows Spark to:\n",
    "    1. Read only 1.25 GB total (3.02x compression)\n",
    "    2. Skip non-2024 year partitions entirely (folder-level skip)\n",
    "    3. Read only the 2 required columns (96.36% IO reduction)\n",
    "    4. Filter BEFORE reading data into memory\n",
    "\n",
    "ENGINEER'S TAKEAWAY:\n",
    "  When optimizing data pipelines, the first question isn't\n",
    "  \"how much compute do we have?\" — it's \"how much data are we\n",
    "  forcing the engine to read?\" Format choice solves that at the source.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 3: INSIGHT 2 — PARTITION PRUNING BREAKS LINEAR SCALING\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INSIGHT 2 — PARTITION PRUNING BEATS LINEAR SCALING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "YOUR DATA:\n",
    "  PP 10M  Filter+Agg (year=2024) → 0.77s\n",
    "  PP 100M Filter+Agg (year=2024) → 1.80s\n",
    "  Scaling factor: 2.34x for 10x data\n",
    "\n",
    "WHAT THIS PROVES:\n",
    "  If Spark processed ALL data linearly, 100M should take ~7.7s (10x of 0.77s).\n",
    "  It actually took 1.80s — only 2.34x.\n",
    "\n",
    "  Why? Because the year=2024 filter is a PARTITION filter, not a row filter.\n",
    "  Spark navigates the folder structure like this:\n",
    "\n",
    "    ecommerce_parquet/\n",
    "      year=2021/  ← SKIPPED entirely (not opened)\n",
    "      year=2022/  ← SKIPPED entirely\n",
    "      year=2023/  ← SKIPPED entirely\n",
    "      year=2024/  ← ONLY this folder is read\n",
    "\n",
    "  In the 100M dataset (10x replicas of 10M), Spark still only reads\n",
    "  the year=2024 partition, which is the same relative fraction.\n",
    "  This is why scaling is sub-linear — the data growth is real,\n",
    "  but the SCAN growth is controlled by partition design.\n",
    "\n",
    "PHYSICAL PLAN PROOF (from your 02_Optimization_Validation output):\n",
    "  PartitionFilters: [isnotnull(year#13525), (year#13525 = 2024)]\n",
    "  ReadSchema: struct<category:string, final_price:double>\n",
    "\n",
    "  These two lines in the Photon physical plan confirm:\n",
    "    → Spark never touched year=2021/2022/2023 folders\n",
    "    → Only 2 columns were deserialized from disk (not all 55)\n",
    "\n",
    "ENGINEER'S TAKEAWAY:\n",
    "  Partition design IS query design. Choosing year/month as partition\n",
    "  keys is not just an organizational decision — it directly determines\n",
    "  whether your queries scale linearly or sub-linearly.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 4: INSIGHT 3 — SHUFFLE IS THE REAL DISTRIBUTED COST\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INSIGHT 3 — SHUFFLE IS WHERE DISTRIBUTED SYSTEMS ACTUALLY HURT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "YOUR DATA (Benchmark Suite):\n",
    "  Stress1 (Hi-Cardinality 4D GroupBy):\n",
    "    PP 10M  → 2.71s\n",
    "    PP 100M → 34.06s   ← 12.6x for 10x data (SUPER-LINEAR)\n",
    "\n",
    "  Stress2 (Window Function):\n",
    "    PP 10M  → 1.15s\n",
    "    PP 100M → 3.68s    ← 3.2x for 10x data (sub-linear)\n",
    "\n",
    "  Nuclear (Repartition 800 + Window + GroupBy):\n",
    "    PP 10M  → 4.91s\n",
    "    PP 100M → 33.03s   ← 6.7x for 10x data\n",
    "\n",
    "WHAT THIS EXPLAINS:\n",
    "  Stress1 went SUPER-LINEAR (12.6x) because:\n",
    "    - groupBy(\"city\", \"category\", \"payment_method\", \"product_id\")\n",
    "    - product_id alone has ~1000 unique values\n",
    "    - 4 dimensions combined = millions of unique group keys\n",
    "    - Every row must find its group across ALL partitions\n",
    "    - Shuffle data volume grows with both row count AND cardinality\n",
    "    - At 100M rows, shuffle writes hundreds of millions of key-value pairs\n",
    "\n",
    "  Stress2 stayed sub-linear (3.2x) because:\n",
    "    - Window partitions on user_id only\n",
    "    - Each user's rows sort independently within their partition\n",
    "    - AQE coalesced small shuffle partitions automatically\n",
    "    - Less inter-node movement = less network cost\n",
    "\n",
    "  Nuclear (6.7x) sits between because:\n",
    "    - Repartition(800) forces a full shuffle first (expensive)\n",
    "    - Window function adds second shuffle\n",
    "    - But groupBy at the end has lower cardinality than Stress1\n",
    "\n",
    "KEY DISTINCTION:\n",
    "  Partition pruning reduces IO (disk problem).\n",
    "  Shuffle reduction reduces network traffic (distributed problem).\n",
    "  They are SEPARATE optimization dimensions — you need both.\n",
    "\n",
    "ENGINEER'S TAKEAWAY:\n",
    "  When a query slows down 12x with 10x data, the culprit is\n",
    "  almost always cardinality explosion in the shuffle layer —\n",
    "  not the data size itself. The fix is either reducing group\n",
    "  dimensions, pre-aggregating, or using broadcast joins.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 5: INSIGHT 4 — AQE MAKES MANUAL TUNING IRRELEVANT\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INSIGHT 4 — AQE ON DATABRICKS SERVERLESS MAKES MANUAL TUNING IRRELEVANT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "YOUR DATA:\n",
    "  PP 100M — 200 shuffle partitions → 5.18s\n",
    "  PP 100M — 400 shuffle partitions → 5.22s\n",
    "  Difference: 0.04 seconds (effectively ZERO)\n",
    "\n",
    "WHAT THIS PROVES:\n",
    "  Textbook Spark tuning says: set shuffle.partitions = 2-3x your cores.\n",
    "  For 100M rows, increasing from 200 to 400 should theoretically help.\n",
    "  In practice, it made zero difference.\n",
    "\n",
    "  Why? Databricks Serverless runs Adaptive Query Execution (AQE) by default.\n",
    "  AQE observes the actual shuffle output at runtime and coalesces partitions\n",
    "  that are smaller than its target size (default 64MB per partition).\n",
    "\n",
    "  So even if you configure 400 partitions, AQE may collapse them to 50\n",
    "  based on actual data distribution — making your config irrelevant.\n",
    "\n",
    "WHAT THIS MEANS FOR PRODUCTION:\n",
    "  On Databricks Serverless, you should trust AQE and NOT manually tune\n",
    "  shuffle.partitions for standard workloads. Manual tuning only matters:\n",
    "    - When AQE is disabled (on-premise clusters, older Spark versions)\n",
    "    - When you have extreme skew AQE can't fix (salting required)\n",
    "    - When your shuffle stage has very specific partition size requirements\n",
    "\n",
    "  The fact that 400 != 200 in performance is direct evidence that\n",
    "  Serverless auto-scaling is absorbing the shuffle cost dynamically.\n",
    "\n",
    "ENGINEER'S TAKEAWAY:\n",
    "  Proving something does NOT matter is as valuable as proving something\n",
    "  does. This test demonstrates you understand the difference between\n",
    "  classic Spark tuning and modern managed Spark environments.\n",
    "  That's a senior-level distinction.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 6: INSIGHT 5 — COLUMN PRUNING + IO QUANTIFICATION\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INSIGHT 5 — COLUMN PRUNING REDUCES IO BY 96.36% (QUANTIFIED)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "YOUR DATA (from 04_Cost_And_IO_Analysis):\n",
    "  Total columns in schema   : 55\n",
    "  Columns used in query     : 2  (category, final_price)\n",
    "  Parquet size (10M)        : 1.25 GB\n",
    "  Estimated scan after pruning: 0.05 GB\n",
    "\n",
    "  Column Scan Fraction     : 3.64%\n",
    "  IO Reduction             : 96.36%\n",
    "\n",
    "  Combined (column + year partition):\n",
    "  Combined Scan Fraction   : 0.9091%\n",
    "  Estimated Scan           : 0.01 GB\n",
    "  Total IO Reduction       : 99.09%\n",
    "\n",
    "WHAT THIS PROVES:\n",
    "  Parquet stores data in COLUMN-MAJOR format (each column in separate\n",
    "  byte ranges within the file). When Spark reads only \"category\" and\n",
    "  \"final_price\", it literally seeks to only those byte ranges on disk\n",
    "  and ignores the other 53 columns completely.\n",
    "\n",
    "  This is visible in your physical plan output:\n",
    "    ReadSchema: struct<category:string, final_price:double>\n",
    "  Only 2 columns appear in ReadSchema — the other 53 were never touched.\n",
    "\n",
    "  For CSV, ReadSchema would always include ALL columns because the\n",
    "  row-based format requires parsing every field to find the ones you want.\n",
    "\n",
    "COST TRANSLATION:\n",
    "  CSV full scan    → $0.0185 per query (at $5/TB)\n",
    "  Parquet full scan→ $0.0061 per query\n",
    "  Optimized query  → $0.0001 per query\n",
    "  Savings vs CSV   → $0.0184 per query\n",
    "\n",
    "  At 10,000 queries/day: ~$67,160/year saved just from format + pruning.\n",
    "\n",
    "ENGINEER'S TAKEAWAY:\n",
    "  Column pruning is \"free\" in Parquet — it requires zero code change.\n",
    "  Just select fewer columns. The Spark optimizer handles the rest.\n",
    "  This is one of the highest ROI optimizations in data engineering.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 7: INSIGHT 6 — PHOTON ENGINE OBSERVATION\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INSIGHT 6 — PHOTON VECTORIZED EXECUTION EXPLAINS SUB-SECOND TIMES\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "OBSERVATION FROM YOUR RUNS:\n",
    "  PP 10M Full Scan Agg → 1.40s    (reading 1.25GB, groupBy, sum)\n",
    "  PP 10M Window Func   → 1.15s    (shuffle + sort per user_id)\n",
    "  PP 10M Filter+Agg    → 0.77s    (partition-pruned read)\n",
    "\n",
    "  These are extraordinarily fast times for 10M rows with complex operations.\n",
    "  Standard open-source Spark on the same hardware would likely take 5-15s.\n",
    "\n",
    "WHY IS PHOTON THIS FAST?\n",
    "  All your explain() outputs end with:\n",
    "    \"The query is fully supported by Photon.\"\n",
    "\n",
    "  Photon is Databricks' native C++ vectorized execution engine that:\n",
    "    1. Processes data in SIMD batches (256/512 rows per CPU instruction)\n",
    "    2. Eliminates JVM overhead (no Java object allocation per row)\n",
    "    3. Uses CPU cache efficiently (columnar batches fit L2/L3 cache)\n",
    "    4. Integrates natively with Parquet columnar format\n",
    "\n",
    "  This is why Window Function on 10M only takes 1.15s.\n",
    "  In pure PySpark (JVM), the same operation typically takes 5-8s\n",
    "  because every row goes through Java object creation + Python GIL.\n",
    "\n",
    "WHAT THIS MEANS FOR YOUR BENCHMARK STORY:\n",
    "  Your results are Databricks Serverless + Photon results.\n",
    "  They represent the production ceiling of what Spark can do.\n",
    "  On-premise Spark without Photon would show LARGER CSV vs Parquet\n",
    "  gaps because Photon partially compensates for CSV inefficiencies.\n",
    "\n",
    "ENGINEER'S TAKEAWAY:\n",
    "  Always document your runtime environment in benchmarks.\n",
    "  \"Spark on Databricks Serverless with Photon\" vs\n",
    "  \"Apache Spark 3.x on EMR\" are NOT comparable baselines.\n",
    "  Your numbers reflect a Photon-accelerated environment.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 8: INSIGHT 7 — count() vs collect() DESIGN CHOICE\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INSIGHT 7 — count() OVER collect() IS A DELIBERATE DESIGN CHOICE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "YOUR BENCHMARK DESIGN:\n",
    "  Every test uses .count() as the terminal action, not .collect().\n",
    "\n",
    "WHY THIS IS CORRECT:\n",
    "  collect() behavior:\n",
    "    - Executes distributed computation on all workers ✓\n",
    "    - Then transfers ALL result rows to the driver node ✗\n",
    "    - For 100M rows, this means: driver receives ~100M Python objects\n",
    "    - Driver memory becomes the bottleneck, not cluster compute\n",
    "    - You'd be measuring Python memory allocation, not Spark performance\n",
    "\n",
    "  count() behavior:\n",
    "    - Executes distributed computation on all workers ✓\n",
    "    - Each worker returns one integer (their local count) ✓\n",
    "    - Driver receives N integers (N = number of partitions) ✓\n",
    "    - Pure distributed compute measurement — no driver bottleneck\n",
    "\n",
    "REAL IMPACT:\n",
    "  If you had used collect() in Stress1 on PP 100M:\n",
    "    - 34.06s → would likely be 5-10 MINUTES\n",
    "    - Or an OOM error on the driver\n",
    "    - Benchmark would measure driver memory pressure, not Spark compute\n",
    "\n",
    "  Your 03_Benchmark_Suite uses count() correctly throughout,\n",
    "  which is why 100M results are measurable in seconds.\n",
    "\n",
    "EDGE CASE — Price Analysis (05) uses show():\n",
    "  show() collects only the top N rows to driver (default 20).\n",
    "  This is correct for displaying results but adds ~6s overhead\n",
    "  because it triggers a full distributed sort before truncating.\n",
    "  That's why all 6 Price Analysis queries take ~6s each —\n",
    "  it's the cost of show()'s sort + collect for display.\n",
    "\n",
    "ENGINEER'S TAKEAWAY:\n",
    "  Benchmark design is engineering. Using collect() in a performance\n",
    "  test is like measuring car speed with the handbrake on.\n",
    "  The choice of terminal action determines what you're actually measuring.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 9: ARCHITECTURE STORY\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ARCHITECTURE FLOW\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "  ┌─────────────────────────────────────────────────────────────┐\n",
    "  │                    YOUR PIPELINE DESIGN                      │\n",
    "  └─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "  [VSCode / Local]\n",
    "       │\n",
    "       │  Generate synthetic 10M row eCommerce dataset\n",
    "       │  55 columns: transactions, users, products, logistics\n",
    "       │  Output: ecommerce_10M_55cols.csv (3.79 GB)\n",
    "       ▼\n",
    "  [Databricks Unity Volume]\n",
    "       │  /Volumes/workspace/default/raw_data/\n",
    "       │  Central storage — accessible by all Databricks notebooks\n",
    "       ▼\n",
    "  [01_Data_Load_and_Conversion]  ──── 93.2s write, 416.2s for 100M\n",
    "       │  CSV → Add year/month columns → Partitioned Parquet (10M)\n",
    "       │  CrossJoin × 10 replicas → Partitioned Parquet (100M)\n",
    "       │  Result: 1.25 GB Parquet vs 3.79 GB CSV (3.02x compression)\n",
    "       ▼\n",
    "  [02_Optimization_Validation]\n",
    "       │  explain(True) on 4 query patterns\n",
    "       │  Confirmed: PartitionFilters, ReadSchema (column pruning)\n",
    "       │  All queries: \"Fully supported by Photon\"\n",
    "       ▼\n",
    "  [03_Benchmark_Suite]           ──── 4 workloads × 3 datasets\n",
    "       │  Task1: Filter+Agg | Stress1: HiCard | Stress2: Window | Nuclear\n",
    "       │  All using count() terminal action\n",
    "       │  Proves: 2x–11x speedup CSV → Parquet\n",
    "       ▼\n",
    "  [04_Cost_And_IO_Analysis]\n",
    "       │  Quantifies: 3.02x compression, 96.36% column pruning IO reduction\n",
    "       │  99.09% combined IO reduction on optimized queries\n",
    "       │  Cost model: $0.0185 → $0.0001 per query\n",
    "       ▼\n",
    "  [05_Price_Optimization_Analysis]  ── 47.08s total, 100M records\n",
    "       │  6 business analytics on 100M dataset\n",
    "       │  Discount impact, elasticity, city revenue, prime segmentation\n",
    "       │  Proves: Production analytics viable at 100M scale in <50s\n",
    "       ▼\n",
    "  [06_Scalability_Test]\n",
    "       │  CSV baseline + 3 tests × 3 datasets + AQE partition comparison\n",
    "       │  Key finding: Partition pruning → 2.34x for 10x data (sub-linear)\n",
    "       │  AQE finding: 200 vs 400 partitions = 0.04s difference\n",
    "       ▼\n",
    "  [07_Engineering_Insights]  (this notebook)\n",
    "       │  All findings synthesized with real numbers\n",
    "       └──▶ GitHub README + Interview Prep\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# SECTION 10: RESUME & INTERVIEW READY\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RESUME BULLET POINTS (copy-paste ready)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "• Engineered distributed Spark pipeline on Databricks Serverless processing\n",
    "  100M+ records across 55-column eCommerce dataset stored in Unity Volume\n",
    "\n",
    "• Achieved up to 11.3x query speedup by converting CSV (3.79GB) to\n",
    "  partitioned Parquet (1.25GB), reducing storage by 66.9% and scan IO by 99%\n",
    "\n",
    "• Validated Spark physical plan optimizations (partition pruning, column\n",
    "  pruning) via explain(True), confirming PartitionFilters and minimal\n",
    "  ReadSchema in Photon execution engine\n",
    "\n",
    "• Demonstrated sub-linear scalability: partition-pruned queries on 100M rows\n",
    "  ran only 2.34x slower than 10M (10x data), proving partition design\n",
    "  eliminates linear data growth from query cost\n",
    "\n",
    "• Quantified AQE behavior on Databricks Serverless: 200 vs 400 shuffle\n",
    "  partitions produced 0.04s difference, proving managed runtime auto-tunes\n",
    "  shuffle without manual intervention\n",
    "\n",
    "• Ran 6 production-grade business analytics on 100M records in 47s total,\n",
    "  demonstrating viability of analytical workloads at scale\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"INTERVIEW TALKING POINTS — DEPTH QUESTIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Q: \"Walk me through how partition pruning works in Spark.\"\n",
    "A: When you write partitionBy(\"year\",\"month\"), Spark creates a folder\n",
    "   hierarchy: year=2024/month=6/part-*.parquet. When you filter on\n",
    "   year=2024, the Catalyst optimizer converts this to a PartitionFilter\n",
    "   before the scan stage. Spark uses the InMemoryFileIndex to list only\n",
    "   matching partition folders, never opening files in other partitions.\n",
    "   In my benchmark, this produced 2.34x scaling for 10x data growth —\n",
    "   because the fraction of data read stays constant.\n",
    "\n",
    "Q: \"Why did your Stress1 (High-Cardinality GroupBy) scale super-linearly?\"\n",
    "A: Stress1 grouped on city × category × payment_method × product_id.\n",
    "   product_id had ~1000 unique values, so the combined key space was\n",
    "   millions of unique groups. In a shuffle-based groupBy, every row must\n",
    "   be hashed to its group key and sent to the correct reducer partition.\n",
    "   At 100M rows, shuffle write volume grows multiplicatively with both\n",
    "   row count and key cardinality — which is why it went 12.6x for 10x data.\n",
    "   The fix would be pre-aggregating to reduce cardinality before groupBy.\n",
    "\n",
    "Q: \"What is AQE and what did your benchmark show about it?\"\n",
    "A: Adaptive Query Execution is Spark's runtime optimizer. It observes\n",
    "   actual shuffle output sizes (not estimates) and dynamically coalesces\n",
    "   small partitions into larger ones. My test compared 200 vs 400 shuffle\n",
    "   partitions on 100M rows — difference was 0.04 seconds. This proves\n",
    "   Databricks Serverless AQE was auto-managing partitions regardless of\n",
    "   my configuration. This is important to know — manual tuning of\n",
    "   spark.sql.shuffle.partitions is largely obsolete in managed environments.\n",
    "\n",
    "Q: \"Why did you use count() instead of collect() in benchmarks?\"\n",
    "A: collect() transfers all rows to the driver, making it a measurement\n",
    "   of Python memory allocation and driver serialization speed — not Spark\n",
    "   distributed compute. For 100M rows, collect() would cause driver OOM\n",
    "   or take minutes. count() triggers full distributed execution but returns\n",
    "   one integer per partition to the driver — pure cluster measurement.\n",
    "   This is standard benchmark design for distributed systems.\n",
    "\n",
    "Q: \"How did you generate the 100M dataset?\"\n",
    "A: I used a crossJoin replication pattern: df_10M.crossJoin(spark.range(10))\n",
    "   followed by dropping the replica column. This creates an exact 10x\n",
    "   replication with identical data distribution, which is critical for\n",
    "   fair scaling tests. Using random data generation would introduce variance\n",
    "   that makes scaling comparisons unreliable. The reproducible cross-join\n",
    "   ensures the only variable is data volume.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"✅ Engineering Insights Complete — Project Is GitHub Ready\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Engineering_Insights",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}