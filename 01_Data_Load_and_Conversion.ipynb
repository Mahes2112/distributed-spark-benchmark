{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff52466-06c9-41be-8093-d1dd16a6dcbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nSTEP 1: Loading 10M CSV\n============================================================\nSchema:\nroot\n |-- transaction_id: integer (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- phone: long (nullable = true)\n |-- email: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- pincode: integer (nullable = true)\n |-- registration_date: date (nullable = true)\n |-- order_id: integer (nullable = true)\n |-- order_date: date (nullable = true)\n |-- order_status: string (nullable = true)\n |-- order_channel: string (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- product_name: string (nullable = true)\n |-- category: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- price_per_unit: double (nullable = true)\n |-- total_price: double (nullable = true)\n |-- discount: double (nullable = true)\n |-- tax: double (nullable = true)\n |-- final_price: double (nullable = true)\n |-- payment_method: string (nullable = true)\n |-- payment_status: string (nullable = true)\n |-- transaction_ref: string (nullable = true)\n |-- bank_name: string (nullable = true)\n |-- card_type: string (nullable = true)\n |-- shipment_id: string (nullable = true)\n |-- warehouse_id: integer (nullable = true)\n |-- courier_partner: string (nullable = true)\n |-- shipment_date: date (nullable = true)\n |-- delivery_date: date (nullable = true)\n |-- delivery_status: string (nullable = true)\n |-- return_requested: string (nullable = true)\n |-- return_reason: string (nullable = true)\n |-- device_type: string (nullable = true)\n |-- browser: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- session_duration: integer (nullable = true)\n |-- traffic_source: string (nullable = true)\n |-- campaign_name: string (nullable = true)\n |-- coupon_code: string (nullable = true)\n |-- loyalty_points: integer (nullable = true)\n |-- customer_rating: integer (nullable = true)\n |-- is_prime_user: string (nullable = true)\n |-- fraud_score: double (nullable = true)\n |-- is_high_risk: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- currency: string (nullable = true)\n |-- emi_option: string (nullable = true)\n |-- gift_wrap: string (nullable = true)\n |-- referral_code: string (nullable = true)\n\nTotal Rows (10M CSV): 10,000,000\n\nSTEP 2: Adding year/month partition columns...\n\nSTEP 3: Writing 10M Partitioned Parquet...\n10M Parquet Write Time: 93.2 seconds\nSaved to: /Volumes/workspace/default/raw_data/ecommerce_parquet\n\nVerification - 10M Parquet Row Count: 10,000,000\n\nSTEP 4: Generating 100M dataset via crossJoin replication...\n100M Row Count: 100,000,000\n\nSTEP 5: Writing 100M Partitioned Parquet...\n100M Parquet Write Time: 416.2 seconds\nSaved to: /Volumes/workspace/default/raw_data/ecommerce_100M_parquet\n\nVerification - 100M Parquet Row Count: 100,000,000\n\n✅ Data Load and Conversion Complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 01_Data_Load_and_Conversion\n",
    "# Purpose: Load 10M CSV → Convert to Partitioned Parquet\n",
    "#          → Generate 100M Partitioned Parquet\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, year, month\n",
    "import time\n",
    "\n",
    "# ── Paths ────────────────────────────────────────────────────\n",
    "CSV_PATH         = \"/Volumes/workspace/default/raw_data/ecommerce_10M_55cols.csv\"\n",
    "PARQUET_10M_PATH = \"/Volumes/workspace/default/raw_data/ecommerce_parquet\"\n",
    "PARQUET_100M_PATH= \"/Volumes/workspace/default/raw_data/ecommerce_100M_parquet\"\n",
    "\n",
    "# ── Step 1: Load CSV ─────────────────────────────────────────\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Loading 10M CSV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(CSV_PATH)\n",
    "\n",
    "print(f\"Schema:\")\n",
    "df_csv.printSchema()\n",
    "print(f\"Total Rows (10M CSV): {df_csv.count():,}\")\n",
    "\n",
    "# ── Step 2: Add Partition Columns ────────────────────────────\n",
    "print(\"\\nSTEP 2: Adding year/month partition columns...\")\n",
    "\n",
    "df_partitioned = df_csv \\\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"year\",  year(col(\"order_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"order_date\")))\n",
    "\n",
    "# ── Step 3: Write 10M Partitioned Parquet ───────────────────\n",
    "print(\"\\nSTEP 3: Writing 10M Partitioned Parquet...\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_partitioned \\\n",
    "    .repartition(100) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(PARQUET_10M_PATH)\n",
    "\n",
    "print(f\"10M Parquet Write Time: {round(time.time() - start, 2)} seconds\")\n",
    "print(f\"Saved to: {PARQUET_10M_PATH}\")\n",
    "\n",
    "# ── Step 4: Verify 10M Parquet ───────────────────────────────\n",
    "df_pp_10M = spark.read.parquet(PARQUET_10M_PATH)\n",
    "print(f\"\\nVerification - 10M Parquet Row Count: {df_pp_10M.count():,}\")\n",
    "\n",
    "# ── Step 5: Generate 100M via CrossJoin ─────────────────────\n",
    "print(\"\\nSTEP 4: Generating 100M dataset via crossJoin replication...\")\n",
    "\n",
    "df_100M = df_pp_10M \\\n",
    "    .crossJoin(spark.range(0, 10).toDF(\"replica\")) \\\n",
    "    .drop(\"replica\") \\\n",
    "    .repartition(400)\n",
    "\n",
    "print(f\"100M Row Count: {df_100M.count():,}\")\n",
    "\n",
    "# ── Step 6: Write 100M Partitioned Parquet ──────────────────\n",
    "print(\"\\nSTEP 5: Writing 100M Partitioned Parquet...\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_100M.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(PARQUET_100M_PATH)\n",
    "\n",
    "print(f\"100M Parquet Write Time: {round(time.time() - start, 2)} seconds\")\n",
    "print(f\"Saved to: {PARQUET_100M_PATH}\")\n",
    "\n",
    "# ── Step 7: Final Verification ───────────────────────────────\n",
    "df_pp_100M = spark.read.parquet(PARQUET_100M_PATH)\n",
    "print(f\"\\nVerification - 100M Parquet Row Count: {df_pp_100M.count():,}\")\n",
    "print(\"\\n✅ Data Load and Conversion Complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Load_and_Conversion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}