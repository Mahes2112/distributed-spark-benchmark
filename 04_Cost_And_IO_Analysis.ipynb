{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a27270b-c37b-4268-8b43-0a3632142902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nSTEP 1: Storage Size Comparison\n============================================================\nCSV Size (GB)     : 3.79 GB\nParquet Size (GB) : 1.25 GB\nCompression Ratio : 3.02x smaller\nStorage Reduction : 66.9%\n\n============================================================\nSTEP 2: Column Pruning IO Reduction Estimate\n============================================================\nTotal Columns            : 55\nSelected Columns         : 2\nColumn Scan Fraction     : 3.64%\nEstimated Scan Size (GB) : 0.05 GB\nIO Reduction via Columns : 96.36%\n\n============================================================\nSTEP 3: Partition Pruning IO Reduction Estimate\n============================================================\nDistinct Years  : 4\nDistinct Months : 12\n\nIf filtering on 1 year:\nPartition Scan Fraction   : 25.0%\nEstimated Scan Size (GB)  : 0.31 GB\nIO Reduction via Year     : 75.0%\n\nIf filtering on 1 year + 1 month:\nPartition Scan Fraction   : 2.0833%\nEstimated Scan Size (GB)  : 0.03 GB\nIO Reduction via Partition: 97.92%\n\n============================================================\nSTEP 4: Combined Column + Partition Reduction\n============================================================\nCombined Scan Fraction      : 0.9091%\nEstimated Scan Size (GB)    : 0.01 GB\nTotal Estimated IO Reduction: 99.09%\n\n============================================================\nSTEP 5: Estimated Cloud Cost Impact\n============================================================\nEstimated Cost (CSV full scan)        : $0.0185\nEstimated Cost (Parquet full scan)    : $0.0061\nEstimated Cost (Optimized query scan) : $0.0001\n\nEstimated Cost Savings vs CSV: $0.0184\n\n✅ Cost & IO Analysis Complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 04_Cost_And_IO_Analysis\n",
    "# Purpose:\n",
    "#   1. Compare CSV vs Parquet storage size\n",
    "#   2. Estimate Column Pruning IO reduction\n",
    "#   3. Estimate Partition Pruning IO reduction\n",
    "#   4. Estimate Combined IO reduction\n",
    "#   5. Translate IO savings into cloud cost impact\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# CONFIG\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "CSV_PATH          = \"/Volumes/workspace/default/raw_data/ecommerce_10M_55cols.csv\"\n",
    "PARQUET_10M_PATH  = \"/Volumes/workspace/default/raw_data/ecommerce_parquet\"\n",
    "\n",
    "TOTAL_COLUMNS     = 55\n",
    "SELECTED_COLUMNS  = 2   # example query: category, final_price\n",
    "\n",
    "COST_PER_TB       = 5   # assumed warehouse cost ($ per TB scanned)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# UTILITY FUNCTIONS\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "def bytes_to_gb(bytes_val):\n",
    "    return bytes_val / (1024**3)\n",
    "\n",
    "def bytes_to_tb(bytes_val):\n",
    "    return bytes_val / (1024**4)\n",
    "\n",
    "def format_gb(bytes_val):\n",
    "    return round(bytes_to_gb(bytes_val), 2)\n",
    "\n",
    "def estimate_cost(bytes_val):\n",
    "    return round(bytes_to_tb(bytes_val) * COST_PER_TB, 4)\n",
    "\n",
    "# Recursive folder size calculation (IMPORTANT FIX)\n",
    "def get_folder_size_recursive(path):\n",
    "    total_size = 0\n",
    "    items = dbutils.fs.ls(path)\n",
    "\n",
    "    for item in items:\n",
    "        if item.isDir():\n",
    "            total_size += get_folder_size_recursive(item.path)\n",
    "        else:\n",
    "            total_size += item.size\n",
    "\n",
    "    return total_size\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# STEP 1: STORAGE SIZE COMPARISON\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Storage Size Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CSV size\n",
    "csv_info = dbutils.fs.ls(CSV_PATH)[0]\n",
    "csv_size_bytes = csv_info.size\n",
    "\n",
    "# Parquet size (recursive)\n",
    "parquet_size_bytes = get_folder_size_recursive(PARQUET_10M_PATH)\n",
    "\n",
    "print(f\"CSV Size (GB)     : {format_gb(csv_size_bytes)} GB\")\n",
    "print(f\"Parquet Size (GB) : {format_gb(parquet_size_bytes)} GB\")\n",
    "\n",
    "if parquet_size_bytes > 0:\n",
    "    compression_ratio = round(csv_size_bytes / parquet_size_bytes, 2)\n",
    "    storage_reduction_pct = round(\n",
    "        (1 - parquet_size_bytes / csv_size_bytes) * 100, 2\n",
    "    )\n",
    "\n",
    "    print(f\"Compression Ratio : {compression_ratio}x smaller\")\n",
    "    print(f\"Storage Reduction : {storage_reduction_pct}%\")\n",
    "else:\n",
    "    print(\"⚠ Parquet size returned 0. Check path.\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# STEP 2: COLUMN PRUNING IO REDUCTION\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Column Pruning IO Reduction Estimate\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "column_fraction = SELECTED_COLUMNS / TOTAL_COLUMNS\n",
    "column_reduction_pct = round((1 - column_fraction) * 100, 2)\n",
    "\n",
    "estimated_scan_bytes_column = parquet_size_bytes * column_fraction\n",
    "\n",
    "print(f\"Total Columns            : {TOTAL_COLUMNS}\")\n",
    "print(f\"Selected Columns         : {SELECTED_COLUMNS}\")\n",
    "print(f\"Column Scan Fraction     : {round(column_fraction * 100, 2)}%\")\n",
    "print(f\"Estimated Scan Size (GB) : {format_gb(estimated_scan_bytes_column)} GB\")\n",
    "print(f\"IO Reduction via Columns : {column_reduction_pct}%\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# STEP 3: PARTITION PRUNING IO REDUCTION\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Partition Pruning IO Reduction Estimate\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_pp_10M = spark.read.parquet(PARQUET_10M_PATH)\n",
    "\n",
    "distinct_years = df_pp_10M.select(\"year\").distinct().count()\n",
    "distinct_months = df_pp_10M.select(\"month\").distinct().count()\n",
    "\n",
    "print(f\"Distinct Years  : {distinct_years}\")\n",
    "print(f\"Distinct Months : {distinct_months}\")\n",
    "\n",
    "# Year-only filter\n",
    "year_fraction = 1 / distinct_years\n",
    "year_reduction_pct = round((1 - year_fraction) * 100, 2)\n",
    "\n",
    "estimated_scan_year_bytes = parquet_size_bytes * year_fraction\n",
    "\n",
    "print(f\"\\nIf filtering on 1 year:\")\n",
    "print(f\"Partition Scan Fraction   : {round(year_fraction * 100, 2)}%\")\n",
    "print(f\"Estimated Scan Size (GB)  : {format_gb(estimated_scan_year_bytes)} GB\")\n",
    "print(f\"IO Reduction via Year     : {year_reduction_pct}%\")\n",
    "\n",
    "# Year + Month filter\n",
    "combined_partition_fraction = 1 / (distinct_years * distinct_months)\n",
    "combined_partition_reduction = round(\n",
    "    (1 - combined_partition_fraction) * 100, 2\n",
    ")\n",
    "\n",
    "estimated_scan_partition_bytes = parquet_size_bytes * combined_partition_fraction\n",
    "\n",
    "print(f\"\\nIf filtering on 1 year + 1 month:\")\n",
    "print(f\"Partition Scan Fraction   : {round(combined_partition_fraction * 100, 4)}%\")\n",
    "print(f\"Estimated Scan Size (GB)  : {format_gb(estimated_scan_partition_bytes)} GB\")\n",
    "print(f\"IO Reduction via Partition: {combined_partition_reduction}%\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# STEP 4: COMBINED COLUMN + PARTITION REDUCTION\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Combined Column + Partition Reduction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "combined_fraction = column_fraction * year_fraction\n",
    "combined_reduction_pct = round((1 - combined_fraction) * 100, 2)\n",
    "\n",
    "estimated_combined_bytes = parquet_size_bytes * combined_fraction\n",
    "\n",
    "print(f\"Combined Scan Fraction      : {round(combined_fraction * 100, 4)}%\")\n",
    "print(f\"Estimated Scan Size (GB)    : {format_gb(estimated_combined_bytes)} GB\")\n",
    "print(f\"Total Estimated IO Reduction: {combined_reduction_pct}%\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# STEP 5: CLOUD COST IMPACT ESTIMATE\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Estimated Cloud Cost Impact\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "csv_cost = estimate_cost(csv_size_bytes)\n",
    "parquet_cost = estimate_cost(parquet_size_bytes)\n",
    "optimized_cost = estimate_cost(estimated_combined_bytes)\n",
    "\n",
    "print(f\"Estimated Cost (CSV full scan)        : ${csv_cost}\")\n",
    "print(f\"Estimated Cost (Parquet full scan)    : ${parquet_cost}\")\n",
    "print(f\"Estimated Cost (Optimized query scan) : ${optimized_cost}\")\n",
    "\n",
    "print(f\"\\nEstimated Cost Savings vs CSV: ${round(csv_cost - optimized_cost, 4)}\")\n",
    "\n",
    "print(\"\\n✅ Cost & IO Analysis Complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Cost_And_IO_Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}