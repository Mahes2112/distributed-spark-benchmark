<div align="center">

```
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— 
â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
â•šâ•â•â•â•â•â• â•šâ•â•â•šâ•â•â•â•â•â•â•   â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•    â•šâ•â•   â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â• 
                                                                                    
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—
 â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
 â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
 â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•   â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•â• â•šâ•â•â•â•â•â•â•šâ•â•  â•šâ•â•
```

# âš¡ Distributed Spark Benchmark Project

### *Engineering-grade performance validation on Databricks Serverless*

[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.x-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)](https://spark.apache.org/)
[![Databricks](https://img.shields.io/badge/Databricks-Serverless-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](https://databricks.com/)
[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org/)
[![Parquet](https://img.shields.io/badge/Apache-Parquet-50ABF1?style=for-the-badge&logo=apache&logoColor=white)](https://parquet.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-22C55E?style=for-the-badge)](LICENSE)

<br/>

> **100,000,000 rows. 5 notebooks. 1 mission â€” prove that format and architecture are everything.**

<br/>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HEADLINE RESULTS                       â”‚
â”‚                                                         â”‚
â”‚   CSV  10M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  ~86s  (baseline) â”‚
â”‚   PP   10M   â–ˆâ–ˆâ–ˆ  ~15s   ğŸ”¥ 5.7x faster                 â”‚
â”‚   PP  100M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  ~30s  10x data â†’ 2x time         â”‚
â”‚                                                         â”‚
â”‚   Partition Pruning â†’ 1.35x for 10x data ğŸ¤¯             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

---

## ğŸ“– Table of Contents

| # | Section |
|---|---------|
| 1 | [ğŸ¯ Project Overview](#-project-overview) |
| 2 | [ğŸ— Architecture](#-architecture) |
| 3 | [ğŸ“ Notebook Structure](#-notebook-structure) |
| 4 | [ğŸ”¬ Optimization Proofs](#-optimization-proofs) |
| 5 | [ğŸ“Š Benchmark Results](#-benchmark-results) |
| 6 | [ğŸ“ˆ Scalability Analysis](#-scalability-analysis) |
| 7 | [âš™ï¸ Tech Stack](#ï¸-tech-stack) |
| 8 | [ğŸš€ How to Run](#-how-to-run) |
| 9 | [ğŸ§  Engineering Insights](#-engineering-insights) |
| 10 | [ğŸ¤ Interview Talking Points](#-interview-talking-points) |

---

## ğŸ¯ Project Overview

This project builds and benchmarks a **production-style distributed Spark data processing pipeline** on Databricks Serverless. The goal is to rigorously compare three dataset formats and sizes across multiple workload types â€” and **prove** every optimization through physical plan analysis.

### What Makes This Different

This isn't a tutorial or a notebook experiment. Every decision here mirrors real production engineering:

- âœ… Used `count()` instead of `collect()` â€” avoids driver bottleneck
- âœ… Validated every optimization via `explain(True)` physical plans
- âœ… Tested 4 workload types from simple aggregation to double-shuffle stress tests
- âœ… Measured step-by-step timing inside every notebook
- âœ… Generated 100M rows via `crossJoin` replication â€” real distributed scale

### Datasets Used

| Dataset | Format | Rows | Partitioned By |
|---------|--------|------|----------------|
| `ecommerce_10M_55cols.csv` | CSV (raw) | 10,000,000 | None |
| `ecommerce_parquet` | Parquet | 10,000,000 | `year`, `month` |
| `ecommerce_100M_parquet` | Parquet | 100,000,000 | `year`, `month` |

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA FLOW                                      â”‚
â”‚                                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  VSCode â”‚â”€â”€â”€â”€â–¶â”‚ Unity Volume â”‚â”€â”€â”€â”€â–¶â”‚  Spark Distributed Engine  â”‚  â”‚
â”‚  â”‚ CSV Gen â”‚     â”‚  raw_data/   â”‚     â”‚   (Databricks Serverless)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                     â”‚                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                    â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  PARTITIONED DATA LAKE                           â”‚  â”‚
â”‚  â”‚                                                                  â”‚  â”‚
â”‚  â”‚  ecommerce_parquet/                                              â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ year=2022/                                                  â”‚  â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ month=1/ â”€â”€ part-00001.parquet                         â”‚  â”‚
â”‚  â”‚  â”‚   â””â”€â”€ month=2/ â”€â”€ part-00001.parquet                         â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ year=2023/                                                  â”‚  â”‚
â”‚  â”‚  â””â”€â”€ year=2024/  â—€â”€â”€ Partition Pruning skips others             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                    â”‚                                                   â”‚
â”‚                    â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  ANALYTICAL QUERIES  â†’  Benchmark  â†’  Physical Plan Proof    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Notebook Structure

```
distributed-spark-benchmark/
â”‚
â”œâ”€â”€ ğŸ“˜ 01_Data_Load_and_Conversion.ipynb
â”‚       Load CSV â†’ Add partitions â†’ Write 10M PP â†’ CrossJoin â†’ Write 100M PP
â”‚
â”œâ”€â”€ ğŸ“˜ 02_Optimization_Validation.ipynb
â”‚       Prove partition pruning + column pruning via explain(True)
â”‚
â”œâ”€â”€ ğŸ“˜ 03_Benchmark_Suite.ipynb
â”‚       4 workloads Ã— 3 datasets = 12 timed comparisons
â”‚
â”œâ”€â”€ ğŸ“˜ 04_Scalability_Test.ipynb
â”‚       10M vs 100M scaling factors + AQE partition tuning
â”‚
â”œâ”€â”€ ğŸ“˜ 05_Engineering_Insights.ipynb
â”‚       Structured observations, architecture recap, interview prep
â”‚
â”œâ”€â”€ ğŸ“„ README.md
â””â”€â”€ ğŸ“„ requirements.txt
```

### What Each Notebook Proves

| Notebook | Question Answered |
|----------|------------------|
| `01` | How was the data generated and stored? |
| `02` | Does Spark actually optimize Parquet queries? |
| `03` | How much faster is Parquet vs CSV across real workloads? |
| `04` | Does it scale near-linearly from 10M â†’ 100M? |
| `05` | What are the engineering takeaways? |

---

## ğŸ”¬ Optimization Proofs

### 1ï¸âƒ£ Partition Pruning

When you filter on a partition column, Spark skips entire folders on disk.

```python
# This triggers PartitionFilters in the physical plan
parquet_df.filter("year = 2024").explain(True)
```

**Physical Plan Output (key line):**
```
PartitionFilters: [isnotnull(year#13), (year#13 = 2024)]
```

> ğŸ’¡ Spark **never reads** year=2022 or year=2023 folders. Pure IO elimination.

---

### 2ï¸âƒ£ Column Pruning

Parquet stores data column-by-column. Selecting 2 of 55 columns means 53 columns are **never read from disk**.

```python
parquet_df.select("category", "final_price").explain(True)
```

**Physical Plan Output (key line):**
```
ReadSchema: struct<category:string, final_price:double>
```

> ğŸ’¡ With 55 columns in the dataset, column pruning alone eliminates **~96% of IO**.

---

### 3ï¸âƒ£ Combined â€” Maximum Optimization

```python
parquet_df \
    .filter("year = 2024")           # â†’ Partition Pruning  (skip folders)
    .select("category", "final_price") # â†’ Column Pruning (skip columns)
    .explain(True)
```

Both optimizations stack. This is Parquet's full power.

---

### 4ï¸âƒ£ Shuffle Tuning

```python
# For 100M workloads â€” increase distributed parallelism
spark.conf.set("spark.sql.shuffle.partitions", 400)
```

> ğŸ’¡ On Databricks Serverless, AQE (Adaptive Query Execution) auto-coalesces shuffle partitions at runtime â€” making manual tuning largely redundant, but understanding the setting is essential.

---

## ğŸ“Š Benchmark Results

### Task 1 â€” Filter + Multi-Column Aggregation

```
Filter: year >= 2023 AND month >= 6 AND discount > 500
GroupBy: city, category
Agg: sum(final_price), sum(quantity)

  CSV  10M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  8.55s
  PP   10M   â–ˆâ–ˆâ–ˆâ–ˆ  1.17s  âœ… 7.3x faster
  PP  100M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  3.88s  (10x data, only 3.3x slower)
```

### Stress 1 â€” High-Cardinality GroupBy (4 dimensions)

```
GroupBy: city, category, payment_method, product_id

  CSV  10M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  8.69s
  PP   10M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  1.90s  âœ… 4.6x faster
  PP  100M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  30.20s  â† shuffle bottleneck
```

> ğŸ’¡ 100M Stress1 takes 30s because `product_id` has millions of unique values â€” the shuffle layer becomes the bottleneck, not disk IO.

### Stress 2 â€” Window Function (Row Number per user)

```
Window: PARTITION BY user_id ORDER BY final_price DESC

  CSV  10M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  5.48s
  PP   10M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  1.04s  âœ… 5.3x faster
  PP  100M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  3.33s  (AQE optimized!)
```

### Nuclear â€” Double Shuffle (Repartition â†’ Window â†’ GroupBy)

```
repartition(800) â†’ row_number() â†’ groupBy(3 dims)
Two full shuffle stages

  CSV  10M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  11.09s
  PP   10M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  4.24s  âœ… 2.6x faster
  PP  100M   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  29.51s
```

### Full Comparison Table

| Workload | CSV 10M | PP 10M | PP 100M | CSVâ†’PP Speedup |
|----------|---------|--------|---------|----------------|
| Task1 Filter+Agg | 8.55s | 1.17s | 3.88s | **7.3x** |
| Stress1 HiCard GroupBy | 8.69s | 1.90s | 30.20s | **4.6x** |
| Stress2 Window Function | 5.48s | 1.04s | 3.33s | **5.3x** |
| Nuclear Double Shuffle | 11.09s | 4.24s | 29.51s | **2.6x** |

---

## ğŸ“ˆ Scalability Analysis

### The Numbers That Tell the Story

```
Data grew 10x (10M â†’ 100M rows)
Expected linear time: 10x slower

Actual results:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Filter+Agg  (Partition Pruning active)  1.35x â† SUB-LINEAR ğŸ¤¯
  Full Scan   (Columnar IO only)          3.54x â† Sub-linear
  Window Fn   (Sort + Shuffle)            3.21x â† Sub-linear
  HiCard GB   (Shuffle bottleneck)       ~16x   â† Super-linear*
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
* product_id high cardinality causes shuffle explosion at 100M
```

### Why Partition Pruning Causes Sub-Linear Scaling

```
10M dataset:  year=2022 | year=2023 | year=2024
               ~~~~~~~~   ~~~~~~~~   âœ… SCANNED

100M dataset: year=2022 | year=2023 | year=2024
               ~~~~~~~~   ~~~~~~~~   âœ… SCANNED

â†’ Filter "year = 2024" reads the SAME partition size
  regardless of total table size!
  That's why 10x data â†’ only 1.35x time.
```

### AQE â€” Shuffle Partition Tuning

```
Manual setting: spark.sql.shuffle.partitions = 200 â†’ 5.18s
Manual setting: spark.sql.shuffle.partitions = 400 â†’ 5.22s
                                                      â”€â”€â”€â”€â”€â”€
                                        Difference:   0.04s â† negligible

â†’ Databricks Serverless AQE is auto-managing partitions at runtime.
  Manual tuning knowledge is still essential for interviews.
```

---

## âš™ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Compute | Databricks Serverless (2XS cluster) |
| Processing Engine | Apache Spark with Photon Engine |
| Storage Format | Apache Parquet (partitioned by year/month) |
| Data Source | CSV (55 columns, 10M rows, 3.79 GB) |
| Storage Layer | Unity Catalog Volume (`/Volumes/workspace/default/raw_data/`) |
| Language | Python 3.10+ (PySpark) |
| Query Optimization | AQE, Partition Pruning, Column Pruning, Predicate Pushdown |

---

## ğŸš€ How to Run

### Prerequisites

```bash
# Databricks workspace with:
# - Unity Catalog enabled
# - Serverless compute available
# - Volume: /Volumes/workspace/default/raw_data/
```

### Step 1 â€” Upload CSV to Unity Volume

Upload your `ecommerce_10M_55cols.csv` to:
```
/Volumes/workspace/default/raw_data/
```

### Step 2 â€” Create Workspace Folder

In Databricks workspace, create folder:
```
Distributed_Spark_Benchmark/
```

### Step 3 â€” Create & Run Notebooks In Order

```
01_Data_Load_and_Conversion    â† Run first, creates all data
02_Optimization_Validation     â† Validates physical plans
03_Benchmark_Suite             â† Core performance comparison
04_Scalability_Test            â† 10M vs 100M analysis
05_Engineering_Insights        â† Summary and documentation
```

> âš ï¸ **Important**: Run `01` completely before any other notebook. It creates the Parquet datasets that all other notebooks read.

### Step 4 â€” Verify Data Created

After running notebook 01, confirm these paths exist:
```
/Volumes/workspace/default/raw_data/ecommerce_parquet/
/Volumes/workspace/default/raw_data/ecommerce_100M_parquet/
```

### Expected Runtime Per Notebook

| Notebook | Estimated Time |
|----------|---------------|
| `01_Data_Load_and_Conversion` | ~8â€“12 min |
| `02_Optimization_Validation` | ~30â€“60 sec |
| `03_Benchmark_Suite` | ~3â€“5 min |
| `04_Scalability_Test` | ~3â€“5 min |
| `05_Engineering_Insights` | ~10 sec |

---

## ğŸ§  Engineering Insights

### Why CSV Is Slow

```
CSV problems:
  1. No schema metadata â†’ inferSchema reads entire file
  2. Row-based â†’ must decode all 55 columns even for 2-column query
  3. No partition structure â†’ full scan on every filter
  4. No predicate pushdown â†’ filter after reading, not before
  5. No encoding â†’ larger file size â†’ more IO
```

### Why Parquet Is Fast

```
Parquet advantages:
  1. Column-based â†’ read only the columns you need
  2. Partition directories â†’ skip entire year/month folders
  3. Encoded + compressed â†’ smaller files â†’ less IO
  4. Row group statistics â†’ skip row groups that can't match filter
  5. Schema embedded â†’ no inferSchema cost
```

### The collect() Problem

```python
# âŒ WRONG for benchmarking
df.filter(...).groupBy(...).collect()  # Pulls ALL data to driver â†’ bottleneck

# âœ… CORRECT
df.filter(...).groupBy(...).count()    # Distributed â†’ returns 1 number
```

### Why High-Cardinality Breaks Linear Scaling

```
Low cardinality:  groupBy("category")        â†’ ~10 unique keys   â†’ small shuffle
High cardinality: groupBy("product_id")      â†’ ~millions of keys â†’ massive shuffle

At 100M rows, the shuffle network transfer becomes the bottleneck.
This is why Stress1 at 100M takes 30s even with Parquet.
```

---

## ğŸ¤ Interview Talking Points

<details>
<summary><strong>Q: What is predicate pushdown?</strong></summary>

Predicate pushdown moves filter conditions as close to the data source as possible â€” ideally into the file scan itself. In Parquet, this means filtering happens at the row group level before data is loaded into memory, drastically reducing the amount of data Spark ever processes.

</details>

<details>
<summary><strong>Q: How did you prove partition pruning?</strong></summary>

I used `explain(True)` on a filtered Parquet DataFrame and verified the physical plan showed `PartitionFilters: [(year = 2024)]`. This confirms Spark is physically skipping non-matching partition directories â€” not just filtering after reading.

</details>

<details>
<summary><strong>Q: Why did 10x more data not take 10x longer?</strong></summary>

Because partition pruning makes scaling sub-linear. When filtering on `year = 2024`, Spark reads only that partition folder. The total table size is irrelevant â€” the scan volume is determined by partition size, not table size. That's why 100M rows with pruning took only 1.35x longer than 10M.

</details>

<details>
<summary><strong>Q: What is AQE and what did you find?</strong></summary>

Adaptive Query Execution is Spark's runtime optimizer that adjusts query plans based on actual data statistics collected during execution. It can coalesce shuffle partitions, handle skew, and switch join strategies at runtime. In my tests, changing from 200 to 400 shuffle partitions made only 0.04s difference â€” proving AQE was managing partition count automatically on Databricks Serverless.

</details>

<details>
<summary><strong>Q: Why did you use count() instead of collect()?</strong></summary>

`collect()` transfers all result rows back to the driver node. For 100M rows, this creates a massive bottleneck â€” you'd be measuring network transfer to driver, not distributed compute performance. `count()` triggers full distributed execution but only returns a single integer, giving a true measure of the distributed processing time.

</details>

<details>
<summary><strong>Q: What caused the 30-second time on Stress1 100M?</strong></summary>

Stress1 groups by 4 dimensions including `product_id`, which has millions of unique values (high cardinality). At 100M rows, this generates an enormous shuffle â€” every unique combination must be moved across the network to the same executor. The bottleneck shifts from disk IO (where Parquet excels) to network shuffle, which scales with data volume regardless of format.

</details>

---

## ğŸ“Š Visual Performance Summary

```
                    PERFORMANCE HEATMAP
                    
              CSV 10M    PP 10M    PP 100M
              â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€
Task1          ğŸ”´ 8.5s   ğŸŸ¢ 1.2s   ğŸŸ¡ 3.9s
Stress1        ğŸ”´ 8.7s   ğŸŸ¢ 1.9s   ğŸ”´ 30.2s
Stress2        ğŸŸ¡ 5.5s   ğŸŸ¢ 1.0s   ğŸŸ¢ 3.3s
Nuclear        ğŸ”´11.1s   ğŸŸ¡ 4.2s   ğŸ”´ 29.5s

ğŸŸ¢ < 2s (Fast)    ğŸŸ¡ 2sâ€“6s (Moderate)    ğŸ”´ > 6s (Slow)
```

---

## ğŸ“Œ Key Takeaways

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. FORMAT IS EVERYTHING                                        â”‚
â”‚     Parquet vs CSV = 5â€“7x improvement on identical hardware    â”‚
â”‚                                                                  â”‚
â”‚  2. PARTITION PRUNING BEATS SCALING                             â”‚
â”‚     10x data â†’ 1.35x time when pruning is active               â”‚
â”‚                                                                  â”‚
â”‚  3. SHUFFLE IS THE REAL ENEMY                                   â”‚
â”‚     High-cardinality groupBy at scale â†’ network becomes limit  â”‚
â”‚                                                                  â”‚
â”‚  4. AQE MAKES SERVERLESS SMART                                  â”‚
â”‚     Runtime optimization > manual tuning on Databricks          â”‚
â”‚                                                                  â”‚
â”‚  5. VALIDATE WITH PHYSICAL PLANS                                â”‚
â”‚     Never assume optimizations work â€” prove them with explain() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

<div align="center">

### Built with âš¡ on Databricks Serverless

*If this project helped you understand distributed data engineering â€” drop a â­*

[![GitHub stars](https://img.shields.io/github/stars/yourusername/distributed-spark-benchmark?style=social)](https://github.com/yourusername/distributed-spark-benchmark)

</div>
